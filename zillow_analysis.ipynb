{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "m7xFp6yJ_z6F"
      },
      "source": [
        "# Project Description\n",
        "School choice in renting vs. buying. The choice between renting versus buying housing has been a highly debated topic in therecent years, especially with high volatility of the prices of real estate.  Zillow the mostused real estate search platform in the US which offers information on activity on realestate and rental listings (in terms of the numbers of views) as well as detailed informationon the area where the property is located including “GreatSchools” rating.  The projectis devoted to evaluate if the quality of schools has an effect of the decision of individualsto buys vs rent housing based on the relative activity on rental and for sale listings. The following are the specific steps this project would take:\n",
        "1.  Identify a market which is going to be the focus of your analysis.  This could be acity, a county or a specific zip code.  Provide motivation for your choice.\n",
        "2.  Provide an approach to collect the relevant data from Zillow on the current rentaland  for  sale  properties  in  your  market  as  well  as  specific  information  on  thoseproperties  including  prices,  property  characteristics  (square  footage,  number  of bedrooms) as well as the information on the school quality and the number of userviews of the property.  Collect the dataset of sufficient size (you determine whichsize is sufficient for your analysis).\n",
        "3.  Provide an approach to make the prices of rental and for sale listings compatible.Note  that  rental  prices  are  per  month  while  the  for  sale  prices  are  for  the  entireproperty purchase.\n",
        "4.  Using the number of user views as a proxy for the relative interest of Zillow users in a specific listing, construct a model that would identify how users select to viewrental listings versus for sale listings and the role the quality of schools plays in thischoice"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "QZqNnM8OBAxp"
      },
      "source": [
        "# High-Level Overview\n",
        "1. Iterate through main Zillow listing pages to collect links to individual properties.\n",
        "2. Iterate through individual house links, clicking in to load school quality data. Scrape both 'for sale' and 'rental' listings. Workflow adapted from\n",
        "https://www.scrapingdog.com/blog/scrape-zillow/ , with substantial adjustments.\n",
        "3. Impute predicted sale price for rental properties from monthly rental cost.\n",
        "4. Apply generalized linear model to scraped data to explore factors affecting home price. We evaluate the hypothesis that nearby school quality positively correlates with price for homes for sale but not rentals."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "-JF4tUY3FeSJ"
      },
      "source": [
        "# Main Technical Challenges\n",
        "1. Zillow applies several anti-scraper techniques, including selenium detection, CAPTCHA, and IP blocking.\n",
        "  1. After setting a mock header and enabling JS rendering, our scraper escaped detection but began hitting an unpassable CAPTCHA after multiple webpage accesses. We need to access all 800 listings individually to load school quality data, so this was not workable.\n",
        "  2. We solved this issue by dynamically loading a fresh IP for each new pageview using a proxy API.\n",
        "2. Even after avoiding CAPTCHA, our scraper would be sporadically blocked from capturing the HTML data about half of the time.\n",
        "  1. Since this issue occured randomly and did not stably fail for any given link, we simply iterated the scraper over the remaining \"bad\" links multiple times to collect the complete dataset.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xgZUkmNBFNmA"
      },
      "outputs": [],
      "source": [
        "!pip install requests\n",
        "!pip install beautifulsoup4\n",
        "!pip install selenium-wire\n",
        "!pip install pandas\n",
        "!pip install requests\n",
        "!pip install httpx parsel loguru\n",
        "!pip install geopy"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "aljrzXZWnyks"
      },
      "source": [
        "# *Webscraper*\n",
        "Capture the listings on all 20 pages displayed for Austin, TX by writing the full HTML to a file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AAIPrCWX-A22"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "import math\n",
        "\n",
        "response = requests.get(\n",
        "  url='https://proxy.scrapeops.io/v1/',\n",
        "  params={\n",
        "      'api_key': 'API-KEY', \n",
        "      'url': 'https://www.zillow.com/glen-allen-va/1_p', ## PerimeterX protected website\n",
        "      'bypass': 'perimeterx'\n",
        "  }\n",
        ")\n",
        "\n",
        "with open(r'dataPages.txt', 'w', encoding=\"utf-8\") as fp:\n",
        "  fp.write(\"%s\\n $$$nextHTML$$$\\n\" % response.text)\n",
        "\n",
        "# compute number of pages\n",
        "  soup = BeautifulSoup(response.text,'html.parser')\n",
        "  num_results = soup.find(\"span\", {\"class\":\"result-count\"}).text\n",
        "  num_results = num_results.replace(' results', '')\n",
        "  num_results = int(num_results.replace(',', ''))\n",
        "  num_pages = int(math.ceil(num_results/40))\n",
        "  print(f'Total Number of Results: {num_results}')\n",
        "  print(f'Total Number of Pages: {num_pages}')\n",
        "\n",
        "  for i in range(2,num_pages+1):\n",
        "    rep = requests.get(\n",
        "        url='https://proxy.scrapeops.io/v1/',\n",
        "        params={\n",
        "          'api_key': 'API-KEY',\n",
        "          'url': 'https://www.zillow.com/glen-allen-va/'+str(i)+'_p/',\n",
        "          'bypass': 'perimeterx'\n",
        "        }\n",
        "    )\n",
        "    fp.write(\"%s\\n $$$nextHTML$$$\\n\" % rep.text)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "J8Po-x2gnsB0"
      },
      "source": [
        "Then, extract the links to each individual property while filtering duplicates."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uM8mKSrt8gP9"
      },
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "\n",
        "links = set()\n",
        "\n",
        "with open('dataPages.txt', 'r') as f: #Change for file location\n",
        "    content = f.read()\n",
        "\n",
        "    soup = BeautifulSoup(content, 'html.parser')\n",
        "\n",
        "    for a in (soup.find_all(\"a\", href=re.compile(\"_zpid\"))):\n",
        "        links.add(a[\"href\"])\n",
        "\n",
        "links = list(links)\n",
        "print(links)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "UI4jsXz_EOAI"
      },
      "source": [
        "# *Data Extraction*"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "jr6-6_QJPOYG"
      },
      "source": [
        "Finally, loop over all individual properties and scrape:\n",
        "\n",
        "*   Score of 3 nearest schools\n",
        "*   Page views\n",
        "*   Page saves (analogous to liking)\n",
        "*   Time on market\n",
        "*   Address\n",
        "*   Square footage\n",
        "*   Price\n",
        "\n",
        "We then join these data in a dataframe and export to CSV for downstream analysis. This separation allowed us to work on data harvesting and the analysis/visualization in parallel using a small subset of the data before the scraper was finalized.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tmDsLoAvEy07"
      },
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "def processHouse(houseFile, df):\n",
        "    obj = {}\n",
        "    with open(houseFile, 'r', encoding='utf-8') as fp:\n",
        "        soup = BeautifulSoup(fp, 'html.parser')\n",
        "\n",
        "        try:\n",
        "            #School score list ordered from ES, MS, HS\n",
        "            obj['schoolscore'] = [str(soup.find_all(\"span\", class_=\"Text-c11n-8-84-0__sc-aiai24-0 gTVYcr\")[-3])[51:-7], str(soup.find_all(\"span\", class_=\"Text-c11n-8-84-0__sc-aiai24-0 gTVYcr\")[-2])[51:-7], str(soup.find_all(\"span\", class_=\"Text-c11n-8-84-0__sc-aiai24-0 gTVYcr\")[-1])[51:-7]]\n",
        "        except:\n",
        "            obj['schoolscore'] = None\n",
        "        try:\n",
        "            obj['views'] = str(soup.find_all(\"dt\")[1])[12:-14]\n",
        "        except:\n",
        "            obj['views'] = None\n",
        "        try:\n",
        "            obj['saves'] = str(soup.find_all(\"dt\")[2])[12:-14]\n",
        "        except:\n",
        "            obj['saves'] = None\n",
        "        try:\n",
        "            obj['timeonmarket'] = str(soup.find_all(\"dt\")[0])[12:-14]\n",
        "        except:\n",
        "            obj['timeonmarket'] = None\n",
        "        try:\n",
        "            obj['addr'] = str(soup.find(\"title\"))[7:-32]\n",
        "        except:\n",
        "            obj['addr'] = None\n",
        "        try:\n",
        "            index = str(soup.find_all(\"meta\")).find(\"$\") + 1\n",
        "            space = str(soup.find_all(\"meta\"))\n",
        "            price = \"\"\n",
        "            while True:\n",
        "              if space[index] != \" \" and space[index] != \".\":\n",
        "                price += str(space[index])\n",
        "                index += 1\n",
        "              else:\n",
        "                break\n",
        "            price = int(price.replace(\",\", \"\"))\n",
        "            obj['price'] = price\n",
        "        except:\n",
        "            obj['price'] = None\n",
        "        try:\n",
        "          index = int(str(soup.find_all(\"meta\")).find(\"Square Feet\")) - 2\n",
        "          solset = str(soup.find_all(\"meta\"))\n",
        "\n",
        "          if index > -1:\n",
        "              comma_counter = 0\n",
        "              sqft = \"\"\n",
        "              while True:\n",
        "                  if solset[index] != \",\" and solset[index] != \" \":\n",
        "                      sqft += str(solset[index])\n",
        "                      index -= 1\n",
        "                  else:\n",
        "                      if solset[index] == \",\":\n",
        "                          if comma_counter < 1:\n",
        "                              comma_counter += 1\n",
        "                              index -= 1\n",
        "                          else:\n",
        "                              break\n",
        "                      else:\n",
        "                          break\n",
        "\n",
        "              obj['sqft'] = print(sqft[::-1])\n",
        "\n",
        "          else:\n",
        "              index = int(str(soup.find_all(\"meta\")).find(\"sq. ft.\")) - 2\n",
        "              if index > -1:\n",
        "                  solset = str(soup.find_all(\"meta\"))\n",
        "                  comma_counter = 0\n",
        "                  sqft = \"\"\n",
        "                  while True:\n",
        "                      solset[index]\n",
        "                      if solset[index] != \",\" and solset[index] != \" \":\n",
        "                          sqft += str(solset[index])\n",
        "                          index -= 1\n",
        "                      else:\n",
        "                          if solset[index] == \",\":\n",
        "                              if comma_counter < 1:\n",
        "                                  comma_counter += 1\n",
        "                                  index -= 1\n",
        "                              else:\n",
        "                                  break\n",
        "                          else:\n",
        "                              break\n",
        "\n",
        "                  obj['sqft'] = print(sqft[::-1])\n",
        "\n",
        "              else:\n",
        "                  index = int(str(soup.find_all(\"meta\")).find(\"Acres\")) - 2\n",
        "                  solset = str(soup.find_all(\"meta\"))\n",
        "                  period_counter = 0\n",
        "                  sqft = \"\"\n",
        "                  while True:\n",
        "                      solset[index]\n",
        "                      if solset[index] != \".\" and solset[index] != \" \":\n",
        "                          sqft += str(solset[index])\n",
        "                          index -= 1\n",
        "                      else:\n",
        "                          if solset[index] == \".\":\n",
        "                              if period_counter < 1:\n",
        "                                  sqft += solset[index]\n",
        "                                  period_counter += 1\n",
        "                                  index -= 1\n",
        "                              else:\n",
        "                                  break\n",
        "                          else:\n",
        "                              break\n",
        "\n",
        "                  obj['sqft'] = print(int(float(sqft[::-1])*43560))\n",
        "        except:\n",
        "            obj['sqft'] = None\n",
        "        obj['rentorbuy'] = 'buy'\n",
        "        df.loc[len(df)] = obj"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AlJUJikKMdPC"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import linecache\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame(data=[], columns=['price', 'rentorbuy', 'sqft','price_per_sqft', 'schoolscore', 'views', 'saves', 'timeonmarket', 'lat', 'long', 'addr'])\n",
        "\n",
        "for filename in tqdm(os.listdir(r'FILEPATH')):\n",
        "    f = os.path.join(r'FILEPATH', filename)\n",
        "    if os.path.isfile(f):\n",
        "        processHouse(f, df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w1Yam-DipGxV"
      },
      "outputs": [],
      "source": [
        "# Lambdas for data cleaning\n",
        "def inter(x):\n",
        "    try:\n",
        "        y = int(x)\n",
        "        return y\n",
        "    except:\n",
        "        return None\n",
        "def inter2(x):\n",
        "    if isinstance(x, str):\n",
        "        return int(x.replace(\",\",\"\"))\n",
        "    else:\n",
        "        return x\n",
        "def inter3(x):\n",
        "    if x[-4:] == \"days\":\n",
        "        return int(x.replace(' days',''))\n",
        "    else:\n",
        "        return int(x.replace(' day',''))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "74ak97iGqe7W"
      },
      "outputs": [],
      "source": [
        "df['sqft'] = df['sqft'].apply(inter)\n",
        "df['price_per_sqft'] = df['price'].div(df['sqft'])\n",
        "# Filter out data with missing elements\n",
        "df_good = df.loc[df['schoolscore'].notna()].loc[df['sqft'].notna()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TH_YPbV4trE5"
      },
      "outputs": [],
      "source": [
        "# Drop rows with missing data for schoolscore\n",
        "for i, row in df_good.iterrows():\n",
        "    if row['schoolscore'][0] not in [0,1,2,3,4,5,6,7,8,9,10,'0','1','2','3','4','5','6','7','8','9','10']:\n",
        "        df_good.drop(i, inplace=True)\n",
        "\n",
        "# Then turn remaining schoolscore entries to ints\n",
        "for i, row in df_good.iterrows():\n",
        "    for j in range(len(row['schoolscore'])):\n",
        "        row['schoolscore'][j] = int(row['schoolscore'][j])\n",
        "\n",
        "df_good['views'] = df_good['views'].apply(inter2)\n",
        "df_good['saves'] = df_good['saves'].apply(inter2)\n",
        "df_good['timeonmarket'] = df_good['timeonmarket'].apply(inter3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 677
        },
        "id": "c-nn_-mVyR08",
        "outputId": "69f27cab-f344-4724-cae4-32a22566d752"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>price</th>\n",
              "      <th>rentorbuy</th>\n",
              "      <th>sqft</th>\n",
              "      <th>price_per_sqft</th>\n",
              "      <th>schoolscore</th>\n",
              "      <th>views</th>\n",
              "      <th>saves</th>\n",
              "      <th>timeonmarket</th>\n",
              "      <th>lat</th>\n",
              "      <th>long</th>\n",
              "      <th>addr</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>715000</td>\n",
              "      <td>buy</td>\n",
              "      <td>1669.0</td>\n",
              "      <td>428.400240</td>\n",
              "      <td>[7, 5, 7]</td>\n",
              "      <td>644</td>\n",
              "      <td>55</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>12305 Black Angus Dr, Austin, TX 78727</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>350000</td>\n",
              "      <td>buy</td>\n",
              "      <td>792.0</td>\n",
              "      <td>441.919192</td>\n",
              "      <td>[5, 2, 4]</td>\n",
              "      <td>1932</td>\n",
              "      <td>20</td>\n",
              "      <td>61</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3209 E 16th St, Austin, TX 78721</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>1299500</td>\n",
              "      <td>buy</td>\n",
              "      <td>3770.0</td>\n",
              "      <td>344.694960</td>\n",
              "      <td>[5, 5, 7]</td>\n",
              "      <td>2028</td>\n",
              "      <td>54</td>\n",
              "      <td>59</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5204 Pony Chase, Austin, TX 78727</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>1350000</td>\n",
              "      <td>buy</td>\n",
              "      <td>2376.0</td>\n",
              "      <td>568.181818</td>\n",
              "      <td>[9, 5, 6]</td>\n",
              "      <td>1297</td>\n",
              "      <td>64</td>\n",
              "      <td>16</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2504 Hartford Rd, Austin, TX 78703</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>875000</td>\n",
              "      <td>buy</td>\n",
              "      <td>2278.0</td>\n",
              "      <td>384.108867</td>\n",
              "      <td>[8, 8, 8]</td>\n",
              "      <td>505</td>\n",
              "      <td>29</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>10816 Redmond Rd, Austin, TX 78739</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>639000</td>\n",
              "      <td>buy</td>\n",
              "      <td>852.0</td>\n",
              "      <td>750.000000</td>\n",
              "      <td>[6, 5, 6]</td>\n",
              "      <td>183</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>360 Nueces St APT 1407, Austin, TX 78701</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>415000</td>\n",
              "      <td>buy</td>\n",
              "      <td>2199.0</td>\n",
              "      <td>188.722146</td>\n",
              "      <td>[6, 4, 3]</td>\n",
              "      <td>409</td>\n",
              "      <td>32</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5601 Apple Orchard Ln, Austin, TX 78744</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>650000</td>\n",
              "      <td>buy</td>\n",
              "      <td>2554.0</td>\n",
              "      <td>254.502741</td>\n",
              "      <td>[5, 5, 4]</td>\n",
              "      <td>425</td>\n",
              "      <td>24</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>10621 Beard Ave, Austin, TX 78748</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>7800000</td>\n",
              "      <td>buy</td>\n",
              "      <td>6374.0</td>\n",
              "      <td>1223.721368</td>\n",
              "      <td>[7, 6, 7]</td>\n",
              "      <td>1589</td>\n",
              "      <td>52</td>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3407 Monte Vista Dr, Austin, TX 78731</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>290000</td>\n",
              "      <td>buy</td>\n",
              "      <td>1440.0</td>\n",
              "      <td>201.388889</td>\n",
              "      <td>[6, 8, 7]</td>\n",
              "      <td>312</td>\n",
              "      <td>17</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2123 Tishomingo Trl, Austin, TX 78734</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>699900</td>\n",
              "      <td>buy</td>\n",
              "      <td>1856.0</td>\n",
              "      <td>377.101293</td>\n",
              "      <td>[3, 4, 3]</td>\n",
              "      <td>908</td>\n",
              "      <td>49</td>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1412 Waterloo Shore Ln #5, Austin, TX 78741</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>499000</td>\n",
              "      <td>buy</td>\n",
              "      <td>1496.0</td>\n",
              "      <td>333.556150</td>\n",
              "      <td>[7, 5, 7]</td>\n",
              "      <td>1831</td>\n",
              "      <td>54</td>\n",
              "      <td>8</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3602 Tyrone Dr, Austin, TX 78759</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>399000</td>\n",
              "      <td>buy</td>\n",
              "      <td>1167.0</td>\n",
              "      <td>341.902314</td>\n",
              "      <td>[7, 6, 7]</td>\n",
              "      <td>841</td>\n",
              "      <td>45</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3845 Ranch Rd #2222-31, Austin, TX 78731</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>1600000</td>\n",
              "      <td>buy</td>\n",
              "      <td>4000.0</td>\n",
              "      <td>400.000000</td>\n",
              "      <td>[9, 9, 7]</td>\n",
              "      <td>3109</td>\n",
              "      <td>213</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>11800 Granite Bay Pl, Austin, TX 78732</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>450000</td>\n",
              "      <td>buy</td>\n",
              "      <td>1823.0</td>\n",
              "      <td>246.845858</td>\n",
              "      <td>[9, 3, 4]</td>\n",
              "      <td>857</td>\n",
              "      <td>49</td>\n",
              "      <td>31</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>8813 Sikes Way, Austin, TX 78747</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>375000</td>\n",
              "      <td>buy</td>\n",
              "      <td>1754.0</td>\n",
              "      <td>213.797035</td>\n",
              "      <td>[3, 3, 3]</td>\n",
              "      <td>1573</td>\n",
              "      <td>112</td>\n",
              "      <td>39</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2504 Elara Dr, Austin, TX 78725</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>575000</td>\n",
              "      <td>buy</td>\n",
              "      <td>1329.0</td>\n",
              "      <td>432.656132</td>\n",
              "      <td>[5, 3, 5]</td>\n",
              "      <td>1110</td>\n",
              "      <td>95</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1653 Chippeway Ln, Austin, TX 78745</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>365000</td>\n",
              "      <td>buy</td>\n",
              "      <td>746.0</td>\n",
              "      <td>489.276139</td>\n",
              "      <td>[6, 4, 3]</td>\n",
              "      <td>788</td>\n",
              "      <td>30</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1300 Newning Ave APT 206, Austin, TX 78704</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>712500</td>\n",
              "      <td>buy</td>\n",
              "      <td>2290.0</td>\n",
              "      <td>311.135371</td>\n",
              "      <td>[6, 4, 5]</td>\n",
              "      <td>2180</td>\n",
              "      <td>127</td>\n",
              "      <td>79</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>6902 Lost Valley, Austin, TX 78745</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>515000</td>\n",
              "      <td>buy</td>\n",
              "      <td>1445.0</td>\n",
              "      <td>356.401384</td>\n",
              "      <td>[7, 9, 8]</td>\n",
              "      <td>2344</td>\n",
              "      <td>106</td>\n",
              "      <td>31</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>11814 Highland Oaks Trl, Austin, TX 78759</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      price rentorbuy    sqft  price_per_sqft schoolscore  views  saves  \\\n",
              "26   715000       buy  1669.0      428.400240   [7, 5, 7]    644     55   \n",
              "27   350000       buy   792.0      441.919192   [5, 2, 4]   1932     20   \n",
              "28  1299500       buy  3770.0      344.694960   [5, 5, 7]   2028     54   \n",
              "29  1350000       buy  2376.0      568.181818   [9, 5, 6]   1297     64   \n",
              "32   875000       buy  2278.0      384.108867   [8, 8, 8]    505     29   \n",
              "33   639000       buy   852.0      750.000000   [6, 5, 6]    183      7   \n",
              "34   415000       buy  2199.0      188.722146   [6, 4, 3]    409     32   \n",
              "35   650000       buy  2554.0      254.502741   [5, 5, 4]    425     24   \n",
              "36  7800000       buy  6374.0     1223.721368   [7, 6, 7]   1589     52   \n",
              "37   290000       buy  1440.0      201.388889   [6, 8, 7]    312     17   \n",
              "38   699900       buy  1856.0      377.101293   [3, 4, 3]    908     49   \n",
              "39   499000       buy  1496.0      333.556150   [7, 5, 7]   1831     54   \n",
              "40   399000       buy  1167.0      341.902314   [7, 6, 7]    841     45   \n",
              "41  1600000       buy  4000.0      400.000000   [9, 9, 7]   3109    213   \n",
              "42   450000       buy  1823.0      246.845858   [9, 3, 4]    857     49   \n",
              "43   375000       buy  1754.0      213.797035   [3, 3, 3]   1573    112   \n",
              "44   575000       buy  1329.0      432.656132   [5, 3, 5]   1110     95   \n",
              "45   365000       buy   746.0      489.276139   [6, 4, 3]    788     30   \n",
              "46   712500       buy  2290.0      311.135371   [6, 4, 5]   2180    127   \n",
              "47   515000       buy  1445.0      356.401384   [7, 9, 8]   2344    106   \n",
              "\n",
              "    timeonmarket  lat  long                                         addr  \n",
              "26             2    0     0       12305 Black Angus Dr, Austin, TX 78727  \n",
              "27            61    0     0             3209 E 16th St, Austin, TX 78721  \n",
              "28            59    0     0            5204 Pony Chase, Austin, TX 78727  \n",
              "29            16    0     0           2504 Hartford Rd, Austin, TX 78703  \n",
              "32             3    0     0           10816 Redmond Rd, Austin, TX 78739  \n",
              "33             2    0     0     360 Nueces St APT 1407, Austin, TX 78701  \n",
              "34             3    0     0      5601 Apple Orchard Ln, Austin, TX 78744  \n",
              "35             2    0     0            10621 Beard Ave, Austin, TX 78748  \n",
              "36             9    0     0        3407 Monte Vista Dr, Austin, TX 78731  \n",
              "37             2    0     0        2123 Tishomingo Trl, Austin, TX 78734  \n",
              "38             9    0     0  1412 Waterloo Shore Ln #5, Austin, TX 78741  \n",
              "39             8    0     0             3602 Tyrone Dr, Austin, TX 78759  \n",
              "40             6    0     0     3845 Ranch Rd #2222-31, Austin, TX 78731  \n",
              "41             3    0     0       11800 Granite Bay Pl, Austin, TX 78732  \n",
              "42            31    0     0             8813 Sikes Way, Austin, TX 78747  \n",
              "43            39    0     0              2504 Elara Dr, Austin, TX 78725  \n",
              "44             3    0     0          1653 Chippeway Ln, Austin, TX 78745  \n",
              "45             3    0     0   1300 Newning Ave APT 206, Austin, TX 78704  \n",
              "46            79    0     0           6902 Lost Valley, Austin, TX 78745  \n",
              "47            31    0     0    11814 Highland Oaks Trl, Austin, TX 78759  "
            ]
          },
          "execution_count": 251,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_good[20:40] # Subset of scraped 'for sale' property data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mgeKnt0TTLW0"
      },
      "outputs": [],
      "source": [
        "df_good.to_csv(r'C:\\Users\\James\\AlgoEcon\\houses.csv')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "y7TMYhqMlI7R"
      },
      "source": [
        "We then repeated the same process to scrape rental data and saved it to a separate CSV. The duplicated analysis has been left out for brevity."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "G1xTyBCCHjSv"
      },
      "source": [
        "This code imports latitude and longitude data for each house given the address using Google's geocoding API. These data are used later for visualization purposes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JoODj62f6eWp"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import codecs\n",
        "from pathlib import Path\n",
        "GOOGLE_API_KEY = Path('AlgoEcon/geocodingAPIKEY.txt').read_text()\n",
        "\n",
        "def extract_lat_long_via_address(address_or_zipcode):\n",
        "    lat, lng = None, None\n",
        "    api_key = GOOGLE_API_KEY\n",
        "    base_url = \"https://maps.googleapis.com/maps/api/geocode/json\"\n",
        "    endpoint = f\"{base_url}?address={address_or_zipcode}&key={api_key}\"\n",
        "    r = requests.get(endpoint)\n",
        "    if r.status_code not in range(200, 299):\n",
        "        return None, None\n",
        "    try:\n",
        "        '''\n",
        "        This try block incase any of our inputs are invalid. This is done instead\n",
        "        of actually writing out handlers for all kinds of responses.\n",
        "        '''\n",
        "        results = r.json()['results'][0]\n",
        "        lat = results['geometry']['location']['lat']\n",
        "        lng = results['geometry']['location']['lng']\n",
        "    except:\n",
        "        pass\n",
        "    return lat, lng"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "575aEGoXI2EO"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "houses = pd.read_csv(\"AlgoEcon/houses.csv\")\n",
        "rentals = pd.read_csv(\"AlgoEcon/rentals.csv\")\n",
        "\n",
        "coords = houses['addr'].apply(extract_lat_long_via_address)\n",
        "houses['lat'] = [row[0] for row in coords]\n",
        "houses[\"long\"] = [row[1] for row in coords]\n",
        "\n",
        "coords = rentals['addr'].apply(extract_lat_long_via_address)\n",
        "rentals['lat'] = [row[0] for row in coords]\n",
        "rentals[\"long\"] = [row[1] for row in coords]\n",
        "\n",
        "houses.to_csv(\"AlgoEcon/housesLATLNG.csv\")\n",
        "rentals.to_csv(\"AlgoEcon/rentalsLATLNG.csv\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "UJDDcFjk8cr3"
      },
      "source": [
        "Here we loop back over \"bad\" links where the scraper failed and try again to capture them before rerunning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5EKsY_f18iyu"
      },
      "outputs": [],
      "source": [
        "links = set()\n",
        "\n",
        "with open('alllinks.txt', 'r') as fp:\n",
        "    lines = fp.readlines()\n",
        "    for line in lines:\n",
        "        links.add(line)\n",
        "links = list(links)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oywGWVdv8kHl"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "import math\n",
        "import linecache\n",
        "\n",
        "bad_links = []\n",
        "\n",
        "for i in range(0, 159):\n",
        "    with open('house' + str(i) + '.txt', 'r') as f:\n",
        "            content = f.readlines()\n",
        "\n",
        "            if len(content) != 0:\n",
        "                if content[0][2:8] == \"status\":\n",
        "                    bad_links.append(i)\n",
        "                else:\n",
        "                    if (content[1][:7] == \"<title>\"):\n",
        "                        continue\n",
        "                    else:\n",
        "                        bad_links.append(i)\n",
        "\n",
        "print(bad_links)\n",
        "\n",
        "for i in range(len(bad_links)):\n",
        "    rep = requests.get(\n",
        "                url='https://proxy.scrapeops.io/v1/',\n",
        "                params={\n",
        "                    'api_key': '<API-KEY>',\n",
        "                    'url': links[bad_links[i]],\n",
        "                    'bypass': 'perimeterx',\n",
        "                    'render_js': 'true',\n",
        "                    'optimize_request':'true',\n",
        "                    'residential': 'true',\n",
        "                    'country': 'us',\n",
        "                }\n",
        "        )\n",
        "\n",
        "    with open(r'data_new\\house' + str(i) +'.txt', 'w', encoding=\"utf-8\") as fp:\n",
        "       fp.write(\"%s\" % rep.text)\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "WRwWbeWqlxjN"
      },
      "source": [
        "The main datasets have now been prepared. Analysis and visualization proceeds in the provided R code."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
